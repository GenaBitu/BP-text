\chapter{Multi-instanční učení}\label{MIL}

Co to je, definice z \cite{dietterich_solving_1997}.

Metody řešení - 3 paradigmata - IS, proč nevyhovuje, BS, ES. Co kdo kdy dělal.

Formalismus z \cite{pevny_using_2016}.

Proč embedded space? (výpočetně méně náročný než BS, neexistence instance-level labelů.)

\null

V současné době existují zhruba tři přístupy k učení z příkladů. Těmi jsou \BPname{učení s učitelem} (\BPenname{supervised learning}), \BPname{učení bez učitele} (\BPenname{unsupervised learning}) a \BPname{zpětnovazební učení} (\BPenname{reinforcement learning}). Učení s učitelem má za cíl naučit se správně zařadit neznámé příklady do tříd, které jsou pro trénovací příklady známé. Učení bez učitele má za cíl naučit se odvodit strukturu zdroje příkladů, kde třídy nejsou známé ani pro trénovací příklady. Zpětnovazební učení se pokouší nalézt zobrazení stavů na akce, kde pro trénovací příklady nejsou známé třídy, avšak existuje opožděná zpětná vazba, kterou lze chápat jako opožděné zařazení do třídy. V nedávné době\footnote{Od zveřejnění článku} \cite{dietterich_solving_1997} navrhli tzv. \BPname{Multi-instanční učení}, kde se trénovací data skládají z mnoha tzv. \BPname{tašek} (\BPenname{bags}), které obsahují mnoho instancí. (viz \cite{zhou_neural_2002})

Multi-instanční učení (\BPenname{multi-instance learning, MIL}), poprvé takto popsané v \cite{dietterich_solving_1997}, je variací metody učení s učitelem, metody pro určení klasifikační funkce z trénovacích dat. Je předpokládána existence vstupních objektů ze vstupního prostoru \( \BPspace X \), výstupních objektů (tříd) z výstupního prostoru \( \BPspace Y \). Tyto třídy avšak nejsou známé ani pro trénovací data. Proto jsou vstupní objekty seskupeny do takzvaných tašek (\BPenname{bags}).

\section{Multimnožiny}

Aby byla možná korektní definice pojmu taška, je nejprve potřeba zavést několik pojmů z teorie množin. Prvním z nich je pojem \BPname{multimnožiny} (\BPenname{multiset}), srov. \cite{knuth_art_1968}.

\begin{define}
	Nechť \( \BPset A \) je množina a \( m : \BPset A \to \BPfield N \).
	Uspořádanou dvojici \( \left( \BPset A, m \right) \) nazýváme \BPname{multimnožinou} nad množinou \( \BPset A \). Pro \( a \in \BPset A \) se \( m \left( a \right) \) nazývá \BPname{multiplicitou} (počtem výskytů) prvku \( a \).
\end{define}

Multimnožina je tedy zobecněním pojmu množiny, ve kterém se prvky mohou vyskytovat vícekrát. Z toho vychází i značení, kdy multimnožina může být definována výčtem prvků, ve kterém je prvek opakován tolikrát, jaká je jeho multiplicita.

\begin{example}
	Multimnožinu \( \left( \left\{ a, b, c \right\}, \left\{ \left( a, 2 \right), \left( b, 3 \right), \left( c, 5 \right) \right\} \right) \) zapisujeme jako \( \left\{ a, a, b, b, b, c, c, c, c, c \right\} \).
\end{example}

\begin{remark}
	Množina je speciálním případem multimnožiny, kde každý prvek má multiplicitu \( 1 \).
\end{remark}

\begin{define}
	Nechť \( \BPset A \) je množina. Multimnožina \( \left( \BPset B, m \right) \) je \BPname{podmultimnožinou} množiny \( \BPset A \), pokud platí \( \BPset B \subset \BPset A \). Značíme \( \left( \BPset B, m \right) \subset \BPset A \).
\end{define}

\begin{define}
	Nechť \( \BPset A \) je množina. Jako \BPname{množinu všech podmultimnožin \( \BPset A \)} označujeme množinu
	\[ \mathcal P^M \left( \BPset A \right) = \left\{ \left( \BPset B, m \right) \middle| \left( \BPset B, m \right) \text{ je multimnožinou } \land \left( \BPset B, m \right) \subset \BPset A \right\} \]
\end{define}

Množina všech podmultimnožin je tedy zobecněním potenční množiny, označované \( 2^{\BPset A} \).

\begin{define}
	Nechť \( \BPset B = \left( \BPset A, m \right) \) je multimnožina, kde \( \BPset A \) je konečná množina. Definujeme \BPname{velikost multimnožiny} \( \BPset B \) jako
	\[ \left| \BPset B \right| = \sum_{a \in \BPset A} m \left( a \right) \]
\end{define}

\section{Multi-instanční učení - definice}

Nyní již lze korektně definovat pojem tašky.

\begin{define}
	Nechť \( \BPspace X \) je prostorem vstupních objektů. Potom \BPname{prostorem tašek} je multimnožina \( \BPspace B \) splňující podmínky \todo{Může instance být ve více taškách?}
	\begin{enumerate}
		\item \( \BPspace B \subset \mathcal P^M \left( \BPspace X \right) \)
		\item \( \left( \forall x \in \BPspace X \right) \left( \exists b \in \BPspace B \right) \left( x \in b \right) \)
	\end{enumerate}
	Prvky prostoru tašek se nazývají \BPname{taškami}.
\end{define}

Cílem mutli-instančního učení je řešit problémy, ve kterých není známá správná třída pro vstupní objety, avšak je známá třída právě pro tašky vstupních objektů. \cite{dietterich_solving_1997} dává za příklad multi-instančního problému následující úlohu:

\begin{example}
	Dveře od skladu v kanceláři mají zámek, od nějž mají klíče všichni zaměstnanci v kanceláři. Každý zaměstananec má svazek klíčů, ze kterých právě jeden je od tohoto sdíleného skladu. V kanceláři je avšak používán tzv. systém generálního klíče, tedy zatímco některým zaměstancům slouží klíč od skladu pouze k otevření skladu, jiní používají stejný klíč nejenom k přístupu ke skladu, ale i k odemčení jedněch, či dokonce vícero dalších dveří. Naší úlohou (například v roli zámečníka) je najít nejobecnější tvar, jaký klíč musí mít, aby odemknul dveře od skladu. To by nám umožnilo inspekcí libovolného klíče určit, zda tento klíč odemkne sklad či ne. Naše práce je ale ztížena tím, že zaměstnanci nejsou ochotní nám ukázat, který klíč z jejich svazku odemyká sklad, pouze nám předají celý svůj svazek klíču. Přístup ke skladu je omezen, tedy nemůžeme žádný z klíčů vyzkoušet. Tedy obecný tvar klíče můžeme odvodit pouze studiem získaných svazků klíčů.
\end{example}

V tomto příkladu mají jednotlivé klíče (resp. jejich popis nějakým vektorem příznaků, \todo{odkaz na vektory příznaků}) funkci vstupních objektů, svazky klíčů jsou pak taškami. Přestože má smysl se u každého klíče ptát, zda odemyká sklad, není tato informace k dispozici pro žádný z klíčů, pouze pro celé svazky. Má také smysl tvrdit o celých svazcích, že odemykají (respektive neodemykají) dveře od skladu, pokud obsahují alespoň jeden (respektive neobsahují žádný) klíč, kterým lze sklad odemknout. Tím je zaveden pojem \BPname{třídy tašky}, který \cite{dietterich_solving_1997} definuje následovně:

\begin{define}\label{baglabel}
	Nechť \( \BPspace X \) je prostorem vstupních objektů, \( \BPspace Y \) je prostorem tříd, kde platí
	\[ \left( \forall x \in \BPspace X \right) \left( \exists ! y_x \in \BPspace Y \right) \]
	Nechť \( \BPspace B \) je prostorem tašek. Nechť je v prostoru \( \BPspace Y \) definována funkce maximum. Potom třídou tašky \( b \in \BPspace B \) je
	\[ y_b = \max_{x \in b} \left( y_x \right) \in \BPspace Y \]
\end{define}

Jak bude ale dále ukázáno, je tato definice ve většině případů přinejmenším problematická, neboť velké množství problémů (včetně problému popsaného v kapitole \ref{problem}) předpokládá, že třídy vstupních objektů nejenom nejsou známé, ale nejsou ani dobře definované.

\section{Metody řešení multi-instančních úloh}

V následující sekci jsou popsány tři převládající přístupy k řešení Multi-instančních problémů, srov. \cite{pevny_using_2016} a \cite{pevny_discriminative_2016}.

\subsection{Paradigma prostoru instancí}
\BPname{Paradigma prostoru instancí}(\BPenname{Instance-space paradigm}) je původní přístup k multi-instančnímu učení tak, jak jej popsal \cite{dietterich_solving_1997}. Předpokládá existenci (neznámých) tříd pro vstupní objekty a využívá definice \ref{baglabel}.
Je předpokládána existence výstupních objektů pro všechny vstupní objekty (odpovídající instancím), přestože tyto hodnoty nejsou známé. Pro každou tašku je pak předpokládána exitence výstupního objektu \( y_b \). Cílem metody je najít klasifikační funkci \( f : \BPspace X \to \BPspace Y \) a posléze určit
\[ y_b = \max_{x \in b} \left( f \left( x \right) \right) \]

\BPname{BP-MIP} (srov. \cite{zhou_neural_2002}) řeší problém binární klasifikace (tedy \( \BPspace Y = \left\{ -1, +1 \right\} \)) a předpokládá výstup nějaké vrstevnaté neuronové sítě (\BPenname{feedforward neural network}) \( o_{ij} \) pro \(j\)-tý vstupní objekt v \(i\)-té tašce. Symbolem \( y_{b_i} \) je označována třída tašky \( b_i \). Dále je pak definována chybová funkce pro vstupní objekty
\[ E_{ij} = \begin{cases}
	0 \quad \text{pro} \quad y_{b_i} = +1 \land o_{ij} \geq 0.5 \\
	0 \quad \text{pro} \quad y_{b_i} = -1 \land o_{ij} < 0.5 \\
	\frac{1}{2} \left( o_{ij} - 0.5 \right)^2 \quad \text{jinak}
\end{cases} \]
Pomocí této definice je definována chybová funkce pro celé tašky jako \footnote{Původní článek chybně v mezích uvádí \( \left| b_j \right| \)}
\[ E_i = \begin{cases}
	\min_{1 \leq j \leq \left| b_i \right|} E_{ij} \quad \text{pro} \quad y_{b_i} = +1 \\
	\max_{1 \leq j \leq \left| b_i \right|} E_{ij} \quad \text{pro} \quad y_{b_i} = -1
\end{cases} \]
A dále globální chybová funkce jako
\[ E = \sum_{i = 1}^{\left| \BPspace B \right|} E_i \]
Díky takto dobře definované chybové funkci lze využít k trénování neuronové sítě \BPname{algoritmus zpětné propagace} (\BPenname{backpropagation}), modifikovaný pro multi-instanční učení (modifikace popsána v \cite{zhou_neural_2002}).

\BPname{EM-DD} (srov. \cite{zhang_em-dd:_2002}) kombinuje EM (\BPenname{Expectation-maximization}) algoritmus (srov. \cite{dempster_maximum_1977}) a DD (\BPenname{Diverse density}) algoritmus (srov. \cite{maron_framework_1998}). EM-DD vychází z nějakého odhadu cílového bodu \( h \) v prostoru vstupních objektů, tuto hypotézu \( h \) poté vylepšuje EM algoritmem. V E-kroku je z každé tašky vybrán jeden vstupní objekt, o němž se předpokládá, že má největší vliv na třídu tašky. V M-kroku je využito algoritmu gradientního vzestupu (\BPenname{gradient ascent}) k nalezení nové hypotézy \( h' \), která maximalizuje \( DD \left( h \right) \) (odpovídající pravděpodobnosti, že \( h \) je skutečným cílovým bodem). Opakováním těchto kroků je hypotéza stále vylepšována.

\todo{MILBoost, MI-SVM}

\subsection{Paradigma prostoru tašek}
Předpoklady úlohy jsou omezeny pouze na existenci výstupních objektů na úrovni tašek. Lze definovat kernel funkci
\begin{equation}
	k : \BPspace B \times \BPspace B \to \BPfield R_0^+
\end{equation}
kterou je možno použít jako metriku při klasifikaci například algoritmem k-nejbližsích sousedů.

\subsection{Paradigma vloženého prostoru}
Předpoklady úlohy jsou (stejně jako u paradigmatu prostoru tašek) omezeny pouze na existenci výstupních objektů na úrovni tašek. Nechť existuje vkládající funkce \( \phi : \BPspace B \to \bar{\BPspace X} \), kde \( \bar{\BPspace X} \) je nějaký vstupní prostor. Díky této funkci lze reprezentovat kažkou tašku vstupním objektem \( \phi \left( b \right) \in \bar{\BPspace X} \) a nasledně lze použít jakýkoliv algoritmus používaný při klasickém učení s učitelem. Pokud \( \BPspace X = \bar{\BPspace X} = \BPfield R^n \), pak je většinou kladena na \( \phi \) podmínka
\begin{equation}
	\left( \exists \psi : \BPfield R^k \to \BPfield R \right) \left( \phi \left( x^{(1)}, \dots, x^{(k)} \right) = \left( \psi \left( x_1^{(1)}, \dots, x_1^{(k)} \right), \dots, \psi \left( x_n^{(1)}, \dots, x_n^{(k)} \right) \right) \right)
\end{equation}

\section{Použitý formalismus}\label{used_formalism}
V následujících částech bude používáno \( \BPspace X = \BPfield R^n \), \( \BPspace Y = \left\{ -1, +1 \right\} \) a \( \BPspace B \subset \bigcup_{k = 1}^{\infty} \left( \BPfield R^n \right) ^k \). Prvky \( \BPspace X \) budou nazývány \textit{vektory příznaků}, prvky \( \BPspace Y \) budou nazývány \textit{značky}. Instance bude považována za \textit{pozitivní}, pokud je pozitivní její značka, a negativní jinak. Při použití definice \ref{baglabel} bude taška považována za pozitivní, pokud obsahuje alespoň jednu pozitivní instanci, a negativní, pokud obsahuje pouze negativní instance. Je použito paradigma vloženého prostoru a dále popsaný formalismus, převzatý z \cite{pevny_using_2016} a \cite{pevny_discriminative_2016}.

Nechť existuje prostor rozdělení pravděpodobnosti \( \BPspace P^{\BPspace X} \). Každá taška \( b \) je pak realizací nějaké pravděpodobnostní funkce \( P \left( p_b, y_b \right) \) kde \( p_b \in \BPspace P^{\BPspace X} \) je rozdělení pravděpodobnosti instancí v \( b \), a \( y_b \in \BPspace Y \) je značka této tašky. Je tedy předpokladem, že pravděpodobnostní funkce závisí i na značce, tj. \( p_+ \neq p_- \) kde \( p_+ \sim P(p, +1) \) a \( p_- \sim P(p, -1) \). Prostor všech tašek lze poté alternativně zapsat jako
\todo{Vyhovuje definici?}\begin{equation}
	\BPspace B = \left\{ \left\{ x_1, x_2, \dots, x_n \right\} \in \BPspace P \left( \BPspace X \right) \middle| \left( \forall i \in \hat n \right) \left( x_i \sim p \in \BPspace P^{\BPspace X} \right) \right\}
\end{equation}

Pokud jsou zvoleny funkce \( k : \BPspace X \to \BPspace X \) a \( g : \bigcup_{k = 1}^{\infty} \BPspace X^k \to \bar{\BPspace X} \), lze vkládající funkci definovat jako
\begin{equation}\label{net_pooling}
	\phi : \BPspace B \to \bar{\BPspace X} : b \mapsto g \left( \left\{ k \left( x \right) \middle| x \in b \right\} \right)
\end{equation}
Za \( g \) lze volit například funkce minimum, maximum, aritmetický průměr. Funkce \( k \) je potom určována algoritmem pro učení s učitelem. \todo{Obrázek z \cite{pevny_using_2016}}\todo{Zapsat lépe}Tedy neuronové síťě určující funkce \( f \) a \( k \) jsou trénovány najednou.
