\chapter{Multi-instanční učení}\label{MIL}

V současné době existují zhruba tři přístupy k učení z příkladů. Těmi jsou \BPname{učení s učitelem} (\BPenname{supervised learning}), \BPname{učení bez učitele} (\BPenname{unsupervised learning}) a \BPname{zpětnovazební učení} (\BPenname{reinforcement learning}). Učení s učitelem má za cíl naučit se správně zařadit neznámé příklady do tříd, které jsou pro trénovací příklady známé. Učení bez učitele má za cíl naučit se odvodit strukturu zdroje příkladů, kde třídy nejsou známé ani pro trénovací příklady. Zpětnovazební učení se pokouší nalézt zobrazení stavů na akce, kde pro trénovací příklady nejsou známé třídy, avšak existuje opožděná zpětná vazba, kterou lze chápat jako opožděné zařazení do třídy. V nedávné době\footnote{Od zveřejnění článku.} \cite{dietterich_solving_1997} navrhli tzv. \BPname{Multi-instanční učení}, kde se trénovací data skládají z mnoha tzv. \BPname{tašek} (\BPenname{bags}), které obsahují mnoho instancí. (viz \cite{zhou_neural_2002})

Multi-instanční učení (\BPenname{multi-instance learning, MIL}), poprvé takto popsané v \cite{dietterich_solving_1997}, je variací metody učení s učitelem, metody pro určení klasifikační funkce z trénovacích dat. Je předpokládána existence vstupních objektů ze vstupního prostoru \( \BPspace X \), výstupních objektů (tříd) z výstupního prostoru \( \BPspace Y \). Tyto třídy avšak nejsou známé ani pro trénovací data. Proto jsou vstupní objekty seskupeny do takzvaných tašek (\BPenname{bags}).

\section{Multimnožiny}

Aby byla možná korektní definice pojmu taška, je nejprve potřeba zavést několik pojmů z teorie množin. Prvním z nich je pojem \BPname{multimnožiny} (\BPenname{multiset}), srov. \cite{knuth_art_1968}.

\begin{define}
	Nechť \( \BPset A \) je množina a \( m : \BPset A \to \BPfield N \).
	Uspořádanou dvojici \( \left( \BPset A, m \right) \) nazýváme \BPname{multimnožinou} nad množinou \( \BPset A \). Pro \( a \in \BPset A \) se \( m \left( a \right) \) nazývá \BPname{multiplicitou} (počtem výskytů) prvku \( a \). Pokud \( a \in \BPset A \), říkáme, že \( a \) je prvkem multimnožiny \( \left( \BPset A, m \right) \) a značíme 
	\[ a \in \left( \BPset A, m \right) \]
\end{define}

Multimnožina je tedy zobecněním pojmu množiny, ve kterém se prvky mohou vyskytovat vícekrát. Z toho vychází i značení, kdy multimnožina může být definována výčtem prvků, ve kterém je prvek opakován tolikrát, jaká je jeho multiplicita.

\begin{example}
	Multimnožinu \( \left( \left\{ a, b, c \right\}, \left\{ \left( a, 2 \right), \left( b, 3 \right), \left( c, 5 \right) \right\} \right) \) zapisujeme jako \( \left\{ a, a, b, b, b, c, c, c, c, c \right\} \).
\end{example}

\begin{remark}
	Množina je speciálním případem multimnožiny, kde každý prvek má multiplicitu \( 1 \).
\end{remark}

\begin{define}
	Nechť \( \BPset A \) je množina. Multimnožina \( \left( \BPset B, m \right) \) je \BPname{podmultimnožinou} množiny \( \BPset A \), pokud platí \( \BPset B \subset \BPset A \). Značíme \( \left( \BPset B, m \right) \subset \BPset A \).
\end{define}

\begin{define}
	Nechť \( \BPset A \) je množina. Jako \BPname{množinu všech podmultimnožin \( \BPset A \)} označujeme množinu
	\[ \mathcal P^M \left( \BPset A \right) = \left\{ \left( \BPset B, m \right) \middle| \left( \BPset B, m \right) \text{ je multimnožinou } \land \left( \BPset B, m \right) \subset \BPset A \right\} \]
\end{define}

Množina všech podmultimnožin je tedy zobecněním potenční množiny, označované \( 2^{\BPset A} \).

\begin{define}
	Nechť \( \BPset B = \left( \BPset A, m \right) \) je multimnožina, kde \( \BPset A \) je konečná množina. Definujeme \BPname{velikost multimnožiny} \( \BPset B \) jako
	\[ \left| \BPset B \right| = \sum_{a \in \BPset A} m \left( a \right) \]
\end{define}

\section{Multi-instanční učení}

Nyní již lze korektně definovat pojem tašky.

\begin{define}
	Nechť \( \BPspace X \) je prostorem vstupních objektů. Potom \BPname{prostorem tašek} je multimnožina \( \BPspace B \) splňující podmínky \todo{Může instance být ve více taškách?}
	\begin{enumerate}
		\item \( \BPspace B \subset \mathcal P^M \left( \BPspace X \right) \)
		\item \( \left( \forall x \in \BPspace X \right) \left( \exists b \in \BPspace B \right) \left( x \in b \right) \)
	\end{enumerate}
	Prvky prostoru tašek se nazývají \BPname{taškami}.
\end{define}

Cílem mutli-instančního učení je řešit problémy, ve kterých není známá správná třída pro vstupní objety, avšak je známá třída právě pro tašky vstupních objektů. \cite{dietterich_solving_1997} dává za příklad multi-instančního problému následující úlohu:

\begin{example}
	Dveře od skladu v kanceláři mají zámek, od nějž mají klíče všichni zaměstnanci v kanceláři. Každý zaměstananec má svazek klíčů, ze kterých právě jeden je od tohoto sdíleného skladu. V kanceláři je avšak používán tzv. systém generálního klíče, tedy zatímco některým zaměstancům slouží klíč od skladu pouze k otevření skladu, jiní používají stejný klíč nejenom k přístupu ke skladu, ale i k odemčení jedněch, či dokonce více dalších dveří. Naší úlohou (například v roli zámečníka) je najít nejobecnější tvar, jaký klíč musí mít, aby odemknul dveře od skladu. To by nám umožnilo inspekcí libovolného klíče určit, zda tento klíč odemkne sklad či ne. Naše práce je ale ztížena tím, že zaměstnanci nejsou ochotní nám ukázat, který klíč z jejich svazku odemyká sklad, pouze nám předají celý svůj svazek klíču. Přístup ke skladu je omezen, tedy nemůžeme žádný z klíčů vyzkoušet. Tedy obecný tvar klíče můžeme odvodit pouze studiem získaných svazků klíčů.
\end{example}

V tomto příkladu mají jednotlivé klíče (resp. jejich popis nějakým vektorem příznaků, \todo{odkaz na vektory příznaků}) funkci vstupních objektů, svazky klíčů jsou pak taškami. Přestože má smysl se u každého klíče ptát, zda odemyká sklad, není tato informace k dispozici pro žádný z klíčů, pouze pro celé svazky. Má také smysl tvrdit o celých svazcích, že odemykají (respektive neodemykají) dveře od skladu, pokud obsahují alespoň jeden (respektive neobsahují žádný) klíč, kterým lze sklad odemknout. Tím je zaveden pojem \BPname{třídy tašky}, který \cite{dietterich_solving_1997} definuje následovně:

\begin{define}\label{baglabel}
	Nechť \( \BPspace X \) je prostorem vstupních objektů, \( \BPspace Y \) je prostorem tříd, kde platí
	\[ \left( \forall x \in \BPspace X \right) \left( \exists ! y_x \in \BPspace Y \right) \]
	Nechť \( \BPspace B \) je prostorem tašek. Nechť je v prostoru \( \BPspace Y \) definována funkce maximum. Potom třídou tašky \( b \in \BPspace B \) je
	\[ y_b = \max_{x \in b} \left( y_x \right) \in \BPspace Y \]
\end{define}

Jak bude ale dále ukázáno, je tato definice ve většině případů přinejmenším problematická, neboť velké množství úloh (včetně úlohy popsané v kapitole \ref{problem}) předpokládá, že třídy vstupních objektů nejenom nejsou známé, ale nejsou ani dobře definované.

\section{Metody řešení multi-instančních úloh}

V následující podkapitole jsou popsány tři převládající přístupy k řešení Multi-instančních problémů. Celá tato sekce vychází z \cite{pevny_using_2016} a \cite{pevny_discriminative_2016}.

\subsection{Paradigma prostoru instancí}
\BPname{Paradigma prostoru instancí} (\BPenname{Instance-space paradigm}) je původní přístup k multi-instančnímu učení tak, jak jej popsal \cite{dietterich_solving_1997}. Předpokládá existenci (neznámých) tříd pro vstupní objekty a využívá definice \ref{baglabel}.
Je předpokládána existence výstupních objektů pro všechny vstupní objekty (odpovídající instancím), přestože tyto hodnoty nejsou známé. Pro každou tašku je pak předpokládána exitence výstupního objektu \( y_b \). Cílem metody je najít klasifikační funkci \( f : \BPspace X \to \BPspace Y \) a posléze určit
\[ y_b = \max_{x \in b} \left( f \left( x \right) \right) \]

\BPname{BP-MIP} (srov. \cite{zhou_neural_2002}) řeší problém binární klasifikace (tedy \( \BPspace Y = \left\{ -1, +1 \right\} \)) a předpokládá výstup nějaké vrstevnaté neuronové sítě (\BPenname{feedforward neural network}) \( o_{ij} \) pro \(j\)-tý vstupní objekt v \(i\)-té tašce. Symbolem \( y_{b_i} \) je označována třída tašky \( b_i \). Dále je pak definována chybová funkce pro vstupní objekty
\[ E_{ij} = \begin{cases}
	0 \quad \text{pro} \quad y_{b_i} = +1 \land o_{ij} \geq 0.5 \\
	0 \quad \text{pro} \quad y_{b_i} = -1 \land o_{ij} < 0.5 \\
	\frac{1}{2} \left( o_{ij} - 0.5 \right)^2 \quad \text{jinak}
\end{cases} \]
Pomocí této definice je definována chybová funkce pro celé tašky jako \footnote{Původní článek chybně v mezích uvádí \( \left| b_j \right| \) .}
\[ E_i = \begin{cases}
	\min_{1 \leq j \leq \left| b_i \right|} E_{ij} \quad \text{pro} \quad y_{b_i} = +1 \\
	\max_{1 \leq j \leq \left| b_i \right|} E_{ij} \quad \text{pro} \quad y_{b_i} = -1
\end{cases} \]
A dále globální chybová funkce jako
\[ E = \sum_{i = 1}^{\left| \BPspace B \right|} E_i \]
Díky takto dobře definované chybové funkci lze využít k trénování neuronové sítě \BPname{algoritmus zpětné propagace} (\BPenname{backpropagation}), modifikovaný pro multi-instanční učení (modifikace popsána v \cite{zhou_neural_2002}).

\BPname{EM-DD} (srov. \cite{zhang_em-dd:_2002}) kombinuje EM (\BPenname{Expectation-maximization}) algoritmus (srov. \cite{dempster_maximum_1977}) a DD (\BPenname{Diverse density}) algoritmus (srov. \cite{maron_framework_1998}). EM-DD vychází z nějakého odhadu cílového bodu \( h \) v prostoru vstupních objektů, tuto hypotézu \( h \) poté vylepšuje EM algoritmem. V E-kroku je z každé tašky vybrán jeden vstupní objekt, o němž se předpokládá, že má největší vliv na třídu tašky. V M-kroku je využito algoritmu gradientního vzestupu (\BPenname{gradient ascent}) k nalezení nové hypotézy \( h' \), která maximalizuje \( DD \left( h \right) \) (odpovídající pravděpodobnosti, že \( h \) je skutečným cílovým bodem). Opakováním těchto kroků je hypotéza stále vylepšována.

\todo{MILBoost, MI-SVM}

\subsection{Paradigma prostoru tašek}
\BPname{Paradigma prostoru tašek} (\BPenname{Bag-space paradigm}) omezuje předpoklady úlohy  pouze na existenci tříd na úrovni tašek a zcela opouští představu existence tříd vstupních objektů a s ní i definici \ref{baglabel}. Přístup pomocí paradigmatu prostoru tašek často definuje nějakou vzdálenostní funkci (\BPenname{distance function}) nebo jádrovou funkci (\BPenname{kernel function}), mající podobu
\[ k : \BPspace B \times \BPspace B \to \BPfield R_0^+ \]
Vzhledem k tomu, že není zaveden pojem třídy vstupního objektu, má hledaná klasifikační funkce tvar \( f : \BPspace B \to \BPspace Y \) a je hledána přímo v této podobě.

\BPname{Citation-kNN} (srov. \cite{wang_solving_2000}) definuje pro tašky \( b_1, b_2 \in \BPspace B \) pojem \BPname{minimální Hausdorffovy vzdálenosti} (\BPenname{minimal Hausdorff distance}) jako
\[ H \left( b_1, b_2 \right) = \min_{x \in b_1} \min_{y \in b_2} \left\Vert x - y \right\Vert \]
Je využit algoritmus k-nejbližších sousedů (srov. \cite{dasarathy_nearest_1991}), modifikovaný o systém tzv. \BPname{citací}, kde kromě k-nejbližších sousedů (zde nazývaných \BPname{reference}) je definováno i c-nejbližších citujících, kde za \( c \)-nejbližších citujících instance \( x \in b \) je považováno
\[ Citers \left( x, c \right) = \left\{ x_i \middle| Rank \left( x_i, x \right) \leq c \land x_i \in b \right\} \]
Obě tyto hodnoty jsou vypočteny právě podle minimální Hausdorffovy vzdálenosti. Tašky jsou pak klasifikovány podle součtu těchto hodnot.

\subsection{Paradigma vloženého prostoru}\label{embedded-space-paradigm}

Při použítí \BPname{paradigmatu vloženého prostoru} (\BPenname{Embedded-space paradigm}) jsou, stejně jako v případě paradigmatu prostoru tašek, předpoklady omezeny pouze na existenci výstupních objektů na úrovni tašek. Je zde zavedena \BPname{vkládající funkce} (\BPenname{embedding function}) \( \phi : \BPspace B \to \bar{\BPspace X} \), kde \( \bar{\BPspace X} \) je nějaký vstupní prostor, který může, ale nemusí být totožný s prostorem \( \BPspace X \). Díky této funkci lze reprezentovat kažkou tašku vstupním objektem \( \phi \left( b \right) \in \bar{\BPspace X} \) a nasledně lze použít jakýkoliv algoritmus používaný při klasickém učení s učitelem. Jedny z nejjednodušších vkládajících funkcí jsou například funkce minimum, maximum či aritmetický průměr. Lze ale vkládající funkci definovat například pomocí neuronové sítě, kterou poté lze trénovat společně s neuronovou sítí definující klasifikační funkci \( f \).

\BPname{MILES} (srov. \todo{To moc nechápu, je to dost nečitelný} \cite{chen_miles:_2006}) předpokládá existenci nějakého slovníku vstupních objektů \( \BPspace D \) a pokouší se definovat vkládající funkci jako míru podobnosti tašky s objekty ve slovníku, tedy jako
\[ \phi : \BPspace B \to \BPfield R^{\left| \BPspace D \right|} \]
definovanou po složkách jako
\[ \phi_i \left( b \right) = \sum_{x \in b} k \left( x, d_i \right) \quad \text{kde} \quad d_i \in \BPspace D \]
kde
\[ k \left( x, d \right) = \begin{cases}
	e^{- \frac{1}{\sigma^2} \left\Vert x - d \right\Vert^2} \quad \text{pokud} \; d \; \text{je nejbližším sousedem} \; x \; \text{v} \; \BPspace D \\
	0 \quad \text{jinak}
\end{cases} \]
Výběr vstupních objektů ve slovníku \( \BPspace D \) je prováděn pomocí algoritmu 1-norm SVM (srov. \cite{zhu_1-norm_2004}), který má ovšem nevýhodu v podobě velké výpočetní náročnosti.

\section{Alternativní stochastický formalismus}
V následující podkapitole je popsán alternativní formalismus pro mutli-instanční učení, převzatý z \cite{muandet_learning_2012} a \cite{pevny_using_2016}.

Pro prostor vstupních objektů \( \BPspace X \) nechť existuje měřitelný prostor \( \left( \BPspace X, \BPspace A \right) \), kde \( \BPspace A \) je \( \sigma \)-algebrou prostoru \( \BPspace X \). Nechť \( \BPspace P^{\BPspace X} \) je množinou všech pravděpodobnostních měr prostoru \( \left( \BPspace X, \BPspace A \right) \). Na každou tašku pak lze pohlížet jako na realizaci nějaké pravděpodobnostní funkce \( P \left( p_b, y_b \right) \), kde \( p_b \in \BPspace P^{\BPspace X} \) je rozdělení pravděpodobnosti instancí v \( b \) a \( y_b \in \BPspace Y \) je třída této tašky. Prostor všech tašek lze zapsat jako
\[ \BPspace B = \left\{ x_i \middle| \left( \forall i \in \hat n \right) \left( x_i \sim p \in \BPspace P^{\BPspace X} \right) \right\} \]
Určovaná klasifikační funkci je v tomto formalismu tvaru \( f : \BPspace P^{\BPspace X} \to \BPspace Y \).

Je tedy předpokladem, že pravděpodobnostní funkce závisí i na třídě, tj. v případě binární klasifikace \( p_+ \neq p_- \) kde \( p_+ \sim P(p, +1) \) a \( p_- \sim P(p, -1) \). V souladu s \cite{dietterich_solving_1997} je dále předpokládáno, že existují vstupní objekty, které nikdy nejsou generovány pravděpodobnostními mírami negativní třídy, z čehož plyne
\[ p_+ \setminus p_- \neq \emptyset \]

V případě paradigmatu prostoru tašek je potřeba definovat jádrovou funkci na prostoru tašek, což je v tomto stochastickém formalismu o něco složitější. Nechť \( k \) je jádrovou funkcí prostoru \( \BPspace X \), tedy \( k : \BPspace X \times \BPspace X \to \BPfield R \). Nechť \( \BPspace H \) je Hilbertovým prostorem funkcí \( \BPspace X \to \BPfield R \). Tento prostor je Hilbertovým prostorem reprodukujícím jádro \( k \). Dále nechť je definované zobrazení \( \mu \) jako
\[ \mu : \BPspace P^{\BPspace X} \to \BPspace H : p \mapsto \int_{\BPspace X} k \left( x, \cdot \right) \mathrm{d}p \left( x \right) \]
Pomocí tohoto zobrazení lze definovat jádrovou funkci \( K : \BPspace P^{\BPspace X} \times \BPspace P^{\BPspace X} \to \BPfield R \) jako
\[ K \left( p, q \right) = \braket{\mu \left( p \right) | \mu \left( q \right)}_{\BPspace H} = \iint k \left( x, y \right) \mathrm{d} p \left( x \right) \mathrm{d} q \left( y \right) \]

\todo{lepší název}
\section{Proč paradigma vloženého prostoru?}
Pro modelování klasifikační funkce úlohy popsané v kapitole \ref{problem} bylo použito paradigma vloženého prostoru. Nemá smysl definovat pro jednotlivé části adresy URL, zda spadají do pozitivní či negativní třídy, třída je v této úloze dobře definována pouze na úrovni celých adres URL. To je důvodem, proč nebylo zvoleno paradigma prostoru instancí. Důvody pro upřednostnění paradigmatu vloženého prostoru před paradigmatem prostoru tašek jsou 2. Prvním z nich je vyšší výpočetní náročnost algoritmů využívajících paradigma prostoru tašek. Vzhledem ke složitosti úlohy a množství dostupných dat je výpočetní složitost natolik důležitá, že ovlivňuje i výběr algoritmu. Druhým důvodem je možnost použití standardních algoritmů pro učení s učitelem při použítí paradigmatu vloženého prostoru.
\begin{remark}
	\cite{pevny_using_2016} poukazuje na podobnost mezi paradigmatem prostoru tašek a paradigmatem vloženého prostoru, kde jádrovou funkci lze chápat jako funkci vkládající do nějakého Hilbertova prostoru a naopak jádrovou funkci lze aproximovat skalárním součinem v Eukleidovském prostoru.
\end{remark}
